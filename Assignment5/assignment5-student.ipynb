{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aead223-1efa-44dd-9725-bbe3c8e847c5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a06189bfced4285098b9b9a19c943a6",
     "grade": false,
     "grade_id": "Header",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Assignment 5 - Decentralized Hyperparameter Learning\n",
    "\n",
    "- **Topic:** Decentralized hyperparameter optimization for Gaussian Processes\n",
    "\n",
    "- **Assessment:** The assignment will go through a pass/fail check.\n",
    "\n",
    "\n",
    "- **Deadline:** 4.4.2025\n",
    "- **Submission: SUBMIT ONLY `assignment5_groupNumber.ipynb` TO BRIGHTSPACE**.\n",
    "\n",
    "\n",
    "## Instructions\n",
    "**Installation:** The implementation is tested with python 3.10.16 and packages including\n",
    "-   scipy 1.14.1\n",
    "-   numpy 1.26.4\n",
    "-   matplotlib 3.10.0\n",
    "-   tqdm 4.66.5 (for a progress bar)\n",
    "\n",
    "Other (not too old) versions will probabily also work.\n",
    "\n",
    "You may not use other packages for algorithm-related calculations.\n",
    "You only need to complete (and submit) this file and please do not change other files.\n",
    "\n",
    "## AI Related Policy\n",
    "We strongly disencourage you to use AI tools for implementation assistance. It is your understanding of the problem that is tested in the final exam.\n",
    "\n",
    "## Information\n",
    "Please fill in your names and student numbers in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eabfcce-b53a-44e0-a381-ce1232d4a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR ANSWER HERE\n",
    "\n",
    "STUDENT_1_NAME = \"Tom Lijding\"\n",
    "STUDENT_1_STUDENT_NUMBER = \"6318037\"\n",
    "\n",
    "STUDENT_2_NAME = \"\"\n",
    "STUDENT_2_STUDENT_NUMBER = \"\"\n",
    "\n",
    "# remove or comment out this line\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f422b9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0bac79dc5d37dca613d58c821e17978e",
     "grade": false,
     "grade_id": "Instructions",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Objectives\n",
    "In previous lectures and assignments, you have learnt that the hyperparameters, e.g., signal variance $\\sigma_f$ and length scale $l$ in the squared exponential (SE) kernel, can impact the GP regression. As such, it is naturally motivated to learn the hyperparameters from the data instead of tuning them heuristically. This can be modeled as a optimization problem where we maximize the likelihood of the data over the hyperparameters which, as discussed in the lecture, can be computationally inefficient due to the inversion of a large data covariance matrix. In this assignment, instead of implementing a low-rank approximation for complexity reduction, we take a distributed approach, i.e., we distribute the data to multiple agents where we run local (smaller) optimizations with some communications such that the local optimizations converge to a centralized solution. More specifically, you will implement a proximal ADMM (pxADMM) algorithm for the GP hyperparmeter learning problem. If you recall, you have already done such hyperparameter learning in Assignment 3 where you used the a second order solver (BFGS) from scipy. In comparison, you will explicitly implement the solver, the iterative updates, your self in this assignment.\n",
    "\n",
    "See literature below for more details. \n",
    "\n",
    "[1] A. Xie, F. Yin, Y. Xu, B. Ai, T. Chen and S. Cui, \"Distributed Gaussian Processes Hyperparameter Optimization for Big Data Using Proximal ADMM,\" in IEEE Signal Processing Letters, vol. 26, no. 8, pp. 1197-1201, Aug. 2019.\n",
    "\n",
    "[2] P. Zhai and R. T. Rajan, \"Distributed Gaussian Process Hyperparameter Optimization for Multi-Agent Systems,\" ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05af5416",
   "metadata": {},
   "source": [
    "## 1. Graph Basics\n",
    "Here we play with the fundamentals of graph theory. We first load our GP field in which the range of our \"playground\" is given. Then we generate some random locations for the agent and see if they form a connected graph based on some communication range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c4ad44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "from tqdm import tqdm\n",
    "from utils import param_array_to_dict, param_dict_to_array, data_generator, visualize_samples_on_field, visualize_sample_allocation, visualize_graphs\n",
    "\n",
    "\n",
    "# load artificial field data\n",
    "gp_data = sio.loadmat('artificial_gp_field.mat')\n",
    "gp_field = gp_data['gp_field'].squeeze()               # true field\n",
    "field_resolution = gp_data['resolution'].squeeze()     # number of grid point in each dimension\n",
    "field_range = gp_data['playground_range'].squeeze() \n",
    "print(\"Our Playground Range is: \", field_range)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c68fbfa",
   "metadata": {},
   "source": [
    "### 1.1 Adjacency Matrix and Connectivity Check\n",
    "Here you will implement a function to calculate the adjacency matrix based on a given communication range, i.e., agents within the communications range can have information exchange and thus have a link (edge) in the graph.Then, a function is provided to you to see if a graph is connected. You can tweak the communication range to verify using the graph visulization function.\n",
    "\n",
    "**Question 1**: What does the adjacency matrix look like if there is a disconnected node?\n",
    "\n",
    "**Answer**: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bac9a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_adjacency_matrix(agent_locations, communication_range):\n",
    "    # agent_locations: [GP_INPUT_DIM, NUM_AGENTS]\n",
    "    # communication_range: float\n",
    "\n",
    "    num_agents = agent_locations.shape[1]\n",
    "    adjacency_matrix = np.zeros((num_agents, num_agents), dtype=int)\n",
    "\n",
    "    ################# YOUR CODE HERE #################\n",
    "\n",
    "    ##################################################\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "def is_graph_connected(adjacency_matrix):\n",
    "\n",
    "    degree_matrix = np.diag(np.sum(adjacency_matrix, axis=1))\n",
    "    laplacian_matrix = degree_matrix - adjacency_matrix\n",
    "\n",
    "    # Compute eigenvalues of the Laplacian matrix\n",
    "    eigenvalues = np.linalg.eigvalsh(laplacian_matrix)\n",
    "\n",
    "    # Count zero eigenvalues\n",
    "    zero_eigenvalues = np.sum(np.isclose(eigenvalues, 0))\n",
    "\n",
    "    # Graph is connected if there is exactly one zero eigenvalue\n",
    "    return zero_eigenvalues == 1\n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "NUM_AGENTS = 5\n",
    "AGENT_COMM_RANGE = 5\n",
    "\n",
    "# generate agent positions [GP_INPUT_DIM, NUM_AGENTS]\n",
    "agent_locations = np.array([0.9*np.random.uniform(field_range[0,0], field_range[0,1], NUM_AGENTS),\n",
    "                            0.9*np.random.uniform(field_range[1,0], field_range[1,1], NUM_AGENTS)])\n",
    "\n",
    "adjacency_matrix = calculate_adjacency_matrix(agent_locations, AGENT_COMM_RANGE)\n",
    "if not is_graph_connected(adjacency_matrix):\n",
    "    print(\"Warning: The graph is not connected.\")\n",
    "else:\n",
    "    print(\"The graph is connected.\")\n",
    "visualize_graphs(agent_locations, adjacency_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf9d9d",
   "metadata": {},
   "source": [
    "### 1.2 Architecture for Decentralized Optimization\n",
    "\n",
    "You have seen from the lecture that the (px)ADMM algorithm requires that all nodes are commonly connected to one data fusion node where the local information is collected and processed, which is known as a decentralized architecture.\n",
    "\n",
    "**Question 2**: what can you say about the adjacency matrix in this case and what are the concerns about this architecture.\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "A variation of the decentralized the architecture is the fully distributed version where there is no fusion node and consensus is reached on the edges. In this assignment, we implement the decentralized architecture and we further assume there is an (invisible) fusion node that is already connected to all agents so we do not have to worry about the connectivity of the underlying graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae1e342",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54f0c8acb92a044d6779802a3044c7b0",
     "grade": false,
     "grade_id": "Q1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Proximal ADMM\n",
    "\n",
    "Now we are going to build our pxADMM algorithm step by step. For debugging purposes, you are recommended to use a small scale setup such as `NUM_AGENTS` = 2 and `NUM_SAMPLES_PER_AGENT` = 100. **Warning**: Larger dataset could take significantly longer and please have the toy version working first. Later you could test the performance of the algorithm given massive data.\n",
    "\n",
    "### 2.1 Simulation Setups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac3921",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1eebbcced18b90089a09a3df84739fce",
     "grade": false,
     "grade_id": "Q1-a-answer",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "GP_INPUT_DIM = 2\n",
    "NUM_GP_HYPERPARAMS = 4\n",
    "STOP_TOLERANCE = 1e-4     # stopping criterion\n",
    "MAX_ITR = 15000\n",
    "\n",
    "# measurement noise\n",
    "noise_std = np.sqrt(0.1)      # noise standard deviation\n",
    "\n",
    "# the true hyperparameters given in the field data\n",
    "# the hyperparameters are stored in a dictionary including\n",
    "gp_hyperparams_true = {\n",
    "    'signal_var': float(gp_data['sigma_f'].item()),         # sigma_f\n",
    "    'length_scale': gp_data['l'].squeeze().astype(float),   # l \n",
    "    'noise_std': float(noise_std)                       \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dcfeae",
   "metadata": {},
   "source": [
    "### 2.2 Generate Measurements\n",
    "\n",
    "Here you will uniformly generate sample locations in the given field range. The `data_generator()` function will give you clean samples at the generated locations. Then you will generate noisy sample using the given noise variance noise_std. The noisy samples together with the sample locations will be divided among the agents. In this assignment, we randomly allocate the same amount of samples to the agents. In more realistic settings, you could also give samples to the agents based on proximity. In the two figures below you will see a contour plot of the given GP field with your samples (the color of the samples should match the background) and the allocation of the samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5a3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_AGENTS = 2\n",
    "NUM_SAMPLES_PER_AGENT = 100\n",
    "\n",
    "\n",
    "def generate_data(gp_field, field_range, field_resolution, num_agents, num_samples_per_agent, noise_std):\n",
    "\n",
    "    # generate samples locations [GP_INPUT_DIM, TOTAL_NUM_SAMPLES]\n",
    "    total_num_samples = num_agents * num_samples_per_agent\n",
    "    ################# YOUR CODE HERE #################\n",
    "    sample_locations = \n",
    "    ##################################################\n",
    "\n",
    "    clean_samples, mesh_x1, mesh_x2 = data_generator(sample_locations, gp_field, field_range, field_resolution)\n",
    "\n",
    "    # generate noisy samples\n",
    "    ################# YOUR CODE HERE #################\n",
    "    sample_noise = \n",
    "    noisy_samples = \n",
    "    ##################################################\n",
    "\n",
    "    # uniformly allocated samples to agents\n",
    "    noisy_samples_reshaped = np.reshape(noisy_samples, (num_samples_per_agent,num_agents), order = 'C')\n",
    "    # clean_samples_reshaped = np.reshape(clean_samples, (num_samples_per_agent,num_agents), order = 'C')\n",
    "    sample_locations_reshaped = np.reshape(sample_locations, (GP_INPUT_DIM, num_samples_per_agent,num_agents), order = 'C')\n",
    "\n",
    "    sample_locations_list = np.split(sample_locations_reshaped, sample_locations_reshaped.shape[2], axis=2)\n",
    "    sample_locations_list = [arr.squeeze(axis=2) for arr in sample_locations_list]\n",
    "    \n",
    "    return sample_locations_reshaped, sample_locations_list, noisy_samples_reshaped, sample_locations, clean_samples, mesh_x1, mesh_x2\n",
    "\n",
    "\n",
    "# generate agent positions [GP_INPUT_DIM, NUM_AGENTS]\n",
    "agent_locations = np.array([0.9*np.random.uniform(field_range[0,0], field_range[0,1], NUM_AGENTS),\n",
    "                            0.9*np.random.uniform(field_range[1,0], field_range[1,1], NUM_AGENTS)])\n",
    "sample_locations_reshaped, sample_locations_list, noisy_samples_reshaped, sample_locations, clean_samples, mesh_x1, mesh_x2 = generate_data(gp_field, field_range, field_resolution, NUM_AGENTS, NUM_SAMPLES_PER_AGENT, noise_std)\n",
    "# visualize the field and samples\n",
    "fig, axs = plt.subplots(1, 2, figsize=(11, 4))\n",
    "contour = visualize_samples_on_field(axs[0], mesh_x1, mesh_x2, gp_field, sample_locations, clean_samples)\n",
    "fig.colorbar(contour, ax=axs[0], label=\"Field Value\")\n",
    "visualize_sample_allocation(axs[1], agent_locations, sample_locations_list, adjacency_matrix)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a417694",
   "metadata": {},
   "source": [
    "### 2.3 Local Computations\n",
    "\n",
    "Here we establish the agent class where the most crucial local updates happens. We first review the pxADMM algorithm and then we proceed to the code implementation.\n",
    "\n",
    "As mentioned, the GP hyperparameter optimiziation can be formulated as a likelihood maximiation. Equivalently, we minimize the negative log-likelihood (NLL) and in the context of ADMM, we minimize the sum of the local NLLs subject to consensus contraints:\n",
    "$$\n",
    "\\begin{align}\n",
    "  & \\underset{\\{\\boldsymbol{\\theta}_n\\}_{n=1}^N,\\mathbf{z}}{\\textrm{min}}\n",
    "  & & \\sum_{n=1}^{N} l_n(\\boldsymbol{\\theta}_n) \\\\\n",
    "  & \\text{subject to}\n",
    "  && \\boldsymbol{\\theta}_n = \\mathbf{z},\\quad n=1,...,N\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "where $n=1,...,N$ is the agent index, $\\boldsymbol{\\theta} = [\\sigma_f, l_1, l_2, \\sigma_\\epsilon]$ are our GP hyperparameters. The local NLL has the form of\n",
    "$$\n",
    "l_n(\\boldsymbol{\\theta}) = \\frac{1}{2}\\left(\\mathbf{y}_n^{\\mathsf{T}}\\mathbf{K}_n^{-1}(\\boldsymbol{\\theta})\\mathbf{y}_n + \\textrm{log}\\left| \\mathbf{K}_n(\\boldsymbol{\\theta})  \\right|\\right)\n",
    "$$\n",
    "in which the kernel matrix $\\mathbf{K}_n$ consists of the SE kernel values plus the noise variance, i.e.,\n",
    "$$\n",
    "k(\\mathbf{x}_i,\\mathbf{x}_j) = \\sigma_f^2\\textrm{exp}\\left[ -\\frac{1}{2}(\\mathbf{x}_i-\\mathbf{x}_j)^{\\mathsf{T}}\\boldsymbol{\\Sigma}^{-1}(\\mathbf{x}_i-\\mathbf{x}_j)   \\right] + \\sigma_\\epsilon^2\\delta(\\mathbf{x}_i-\\mathbf{x}_j)\n",
    "$$\n",
    "where $\\boldsymbol{\\Sigma} = \\textrm{diag}(l_1^2,l_2^2)$. Note that in this case we use a different length-scale for different GP input dimension, which is a more general case.\n",
    "\n",
    "Then we could perform the standard ADMM updates from the augmented Lagrangian:\n",
    "$$\n",
    "    \\mathcal{L}\\left( \\boldsymbol{\\theta}_1,...,\\boldsymbol{\\theta}_N, \\mathbf{z}, \\boldsymbol{\\lambda}_1,..., \\boldsymbol{\\lambda}_N\\right) = \\sum_{n=1}^{N}\\left( l_n(\\boldsymbol{\\theta}_n)+ \\boldsymbol{\\lambda}_n^{\\mathsf{T}}(\\boldsymbol{\\theta}_n-\\mathbf{z}) + \\frac{\\rho}{2}\\left\\| \\boldsymbol{\\theta}_n-\\mathbf{z}\\right\\|_2^2\\right)\n",
    "$$\n",
    "where $\\rho$ is a positive constant and $\\boldsymbol{\\lambda}_1,..., \\boldsymbol{\\lambda}_N$ are the dual variables.\n",
    "\n",
    "The ADMM update equations (z-update, primal update, and dual update) are:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{z}^{t+1} &= \\underset{\\mathbf{z}}{\\textrm{argmin}} \\left( \\sum_{n=1}^{N}-{\\boldsymbol{\\lambda}^t_n}^{\\mathsf{T}}\\mathbf{z} + \\frac{\\rho}{2}\\left\\| \\boldsymbol{\\theta}^t_n-\\mathbf{z}\\right\\|_2^2\\right)\\\\\n",
    "\\boldsymbol{\\theta}_n^{t+1} & = \\underset{\\boldsymbol{\\theta}_n}{\\textrm{argmin}} \\left(  l_n(\\boldsymbol{\\theta}_n)+ {\\boldsymbol{\\lambda}^t_n}^{\\mathsf{T}}\\boldsymbol{\\theta}_n + \\frac{\\rho}{2}\\left\\| \\boldsymbol{\\theta}_n-\\mathbf{z}^{t+1}\\right\\|_2^2\\right)\\\\\n",
    "\\boldsymbol{\\lambda}_n^{t+1} &= \\boldsymbol{\\lambda}_n^{t} + \\rho(\\boldsymbol{\\theta}_n^{t+1}-\\mathbf{z}^{t+1})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e053d267",
   "metadata": {},
   "source": [
    "**Question 3**: What is the solution to the z-update?\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "\n",
    "The primal update contains the NLL and a analytical update is difficult to find. As such, we linearize the NLL using the Taylor expansion\n",
    "$$\n",
    "    l_n(\\boldsymbol{\\theta}_n) \\approx l_n(\\mathbf{z}^{t+1}) + \\nabla^{\\mathsf{T}} l_n(\\mathbf{z}^{t+1})(\\boldsymbol{\\theta}_n-\\mathbf{z}^{t+1})\n",
    "$$\n",
    "such that the primal update becomes\n",
    "$$\n",
    "\\boldsymbol{\\theta}_n^{t+1}  = \\underset{\\boldsymbol{\\theta}_n}{\\textrm{argmin}} \\left(  l_n(\\mathbf{z}^{t+1}) + \\nabla^{\\mathsf{T}} l_n(\\mathbf{z}^{t+1})(\\boldsymbol{\\theta}_n-\\mathbf{z}^{t+1})+ {\\boldsymbol{\\lambda}^t_n}^{\\mathsf{T}}\\boldsymbol{\\theta}_n + \\frac{\\rho+L}{2}\\left\\| \\boldsymbol{\\theta}_n-\\mathbf{z}^{t+1}\\right\\|_2^2\\right)\n",
    "$$\n",
    "where there is an additional quadratic term $\\frac{L}{2}\\left\\| \\boldsymbol{\\theta}_n-\\mathbf{z}^{t+1}\\right\\|_2^2$ to increase stability and $L$ is a constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd18372d",
   "metadata": {},
   "source": [
    "**Question 4**: Give the solution to the above proximal primal update: (hint: many terms are w.r.t $\\mathbf{z}^{t+1}$ and does not impact your optimial $\\boldsymbol{\\theta}_n$\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0e0a13",
   "metadata": {},
   "source": [
    "As you noticed, the term $\\nabla l_n(\\mathbf{z}^{t+1})$ arises which makes the computation of the gradients necessary, but you don't have to implement this your self. Now let's resume the coding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a9af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, agent_id, position, noisy_samples, sample_locations, sample_size, gp_input_dim = 2):\n",
    "        self.id = agent_id\n",
    "        self.position = position\n",
    "        self.input_dim = gp_input_dim\n",
    "\n",
    "        # dataset\n",
    "        self.noisy_samples_local = noisy_samples\n",
    "        self.sample_locations_local = sample_locations\n",
    "        self.sample_size = sample_size\n",
    "        assert(noisy_samples.shape[0] == sample_size)\n",
    "\n",
    "    def initialize_pxADMM(self):\n",
    "        \n",
    "        self.gp_hyperparams_local = {\n",
    "                                        'signal_var': 1.,\n",
    "                                        'length_scale': np.array([2., 2.]),\n",
    "                                        'noise_std': 1.\n",
    "                                    }\n",
    "        self.objective_local = 0\n",
    "\n",
    "        num_hyperparams = len(param_dict_to_array(self.gp_hyperparams_local))\n",
    "        self.pxADMM_params = {\n",
    "                            'dual_var': np.ones(num_hyperparams),       # dual variable\n",
    "                            'rho': 400,                             # weighting parameter\n",
    "                            'L': 4000,                              # Lipschitz constant\n",
    "                        }\n",
    "\n",
    "    def pxADMM_local_update(self):\n",
    "        \n",
    "        # compute gradients\n",
    "        gradients, self.objective_local = self.compute_gradient()\n",
    "\n",
    "        # update hyperparameters\n",
    "        ################# YOUR CODE HERE #################\n",
    "        new_signal_var = \n",
    "        new_l = \n",
    "        new_noise_std = \n",
    "        ##################################################\n",
    "        \n",
    "\n",
    "        # keep an old copy of the hyperparameters before updating\n",
    "        old_gp_hyperparams = copy.deepcopy(self.gp_hyperparams_local)\n",
    "        self.gp_hyperparams_local['signal_var'] = new_signal_var\n",
    "        self.gp_hyperparams_local['length_scale'] = new_l\n",
    "        self.gp_hyperparams_local['noise_std'] = new_noise_std\n",
    "\n",
    "        # raise NotImplementedError()\n",
    "        # update the dual variable \n",
    "        ################# YOUR CODE HERE #################\n",
    "        self.pxADMM_params['dual_var'] += \n",
    "        ##################################################\n",
    "\n",
    "    def compute_gradient(self):\n",
    "        from scipy.linalg import cholesky, inv, det\n",
    "        from scipy.spatial.distance import cdist\n",
    "\n",
    "        gradients = {}\n",
    "        \n",
    "        # raise NotImplementedError()\n",
    "        K_n = self.gp_hyperparams_local['noise_std']**2 * np.eye(self.sample_size)\n",
    "\n",
    "        scaled_sample_locations = np.copy(np.diag(1 / self.gp_hyperparams_local['length_scale'] ) @ self.sample_locations_local)\n",
    "        squared_distance_mat = cdist(scaled_sample_locations.T, scaled_sample_locations.T, 'sqeuclidean')\n",
    "        K_s = self.gp_hyperparams_local['signal_var']**2 * np.exp(-0.5 * squared_distance_mat)\n",
    "        K = K_s + K_n\n",
    "\n",
    "        K_inv = inv(K+1e-10*np.eye(np.shape(K)[0]))\n",
    "        constant_1 = K_inv - K_inv @ np.outer(self.noisy_samples_local, self.noisy_samples_local) @ K_inv.T\n",
    "        objective = self.noisy_samples_local.T @ K_inv @ self.noisy_samples_local + np.log(det(K))\n",
    "\n",
    "        # gradient with respect to signal_var\n",
    "        K_div_signal_var= 2 / self.gp_hyperparams_local['signal_var'] * K_s\n",
    "        partial_signal_var= 0.5 * np.trace(constant_1 @ K_div_signal_var)\n",
    "\n",
    "        # gradient with respect to length_scale\n",
    "        partial_length_scale = np.zeros(self.input_dim)\n",
    "        \n",
    "        squared_distance_dim_1 = cdist(self.sample_locations_local[[0],:].T, self.sample_locations_local[[0],:].T, 'sqeuclidean')\n",
    "        squared_distance_dim_2 = cdist(self.sample_locations_local[[1],:].T, self.sample_locations_local[[1],:].T, 'sqeuclidean')\n",
    "\n",
    "        K_div_l_dim_1 = squared_distance_dim_1 * K_s * self.gp_hyperparams_local['length_scale'][0]**(-3)\n",
    "        K_div_l_dim_2 = squared_distance_dim_2 * K_s * self.gp_hyperparams_local['length_scale'][1]**(-3)\n",
    "        \n",
    "        partial_length_scale[0] = 0.5 * np.trace(constant_1 @ K_div_l_dim_1)\n",
    "        partial_length_scale[1] = 0.5 * np.trace(constant_1 @ K_div_l_dim_2)\n",
    "\n",
    "        # gradient with respect to noise_var\n",
    "        K_div_noise_std = 2 * np.sqrt(K_n)\n",
    "        partial_noise_std = 0.5 * np.trace(constant_1 @ K_div_noise_std)\n",
    "\n",
    "\n",
    "        gradients['partial_signal_var'] = partial_signal_var\n",
    "        gradients['partial_l'] = partial_length_scale\n",
    "        gradients['partial_noise_std'] = partial_noise_std\n",
    "\n",
    "        return gradients, objective\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80854dc",
   "metadata": {},
   "source": [
    "### 1.5 Run pxADMM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c89fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pxADMM(agent_locations, noisy_samples_reshaped, sample_locations_reshaped, NUM_SAMPLES_PER_AGENT, NUM_AGENTS, MAX_ITR, STOP_TOLERANCE):\n",
    "\n",
    "    # create some logs for visualization\n",
    "    gp_hyperparams_log = {\n",
    "        'signal_var': [],\n",
    "        'length_scale': [],\n",
    "        'noise_std': [],\n",
    "    }\n",
    "    objective_log = []      # log the negative log likelihood\n",
    "    params_update_log = []  # log the parameters update\n",
    "\n",
    "    # initialize agents\n",
    "    agents = [Agent(i, agent_locations[:, i], noisy_samples_reshaped[:,i],sample_locations_reshaped[:,:,i],NUM_SAMPLES_PER_AGENT) for i in range(NUM_AGENTS)]\n",
    "\n",
    "    # initialize hyperparameters and pxADMM\n",
    "    for agent in agents: agent.initialize_pxADMM()\n",
    "    print('true hyperparameters:', gp_hyperparams_true, 'initialized at:', agents[0].gp_hyperparams_local)\n",
    "    print('pxADMM settings:', agents[0].pxADMM_params)\n",
    "\n",
    "\n",
    "    itr = 0\n",
    "    is_converged = False\n",
    "    old_gp_hyperparams_global = param_dict_to_array(copy.deepcopy(agents[0].gp_hyperparams_local))\n",
    "\n",
    "\n",
    "    with tqdm(total=MAX_ITR) as pbar:\n",
    "\n",
    "        while itr < MAX_ITR and not is_converged:\n",
    "            itr += 1\n",
    "\n",
    "            # data fusion, central node, z-update\n",
    "            gp_hyperparams_global = np.zeros(NUM_GP_HYPERPARAMS)\n",
    "\n",
    "            ################# YOUR CODE HERE #################\n",
    "            for agent in agents:\n",
    "                \n",
    "\n",
    "            ##################################################\n",
    "            \n",
    "            # local update\n",
    "            for agent in agents:\n",
    "                agent.gp_hyperparams_local = param_array_to_dict(copy.deepcopy(gp_hyperparams_global))      # distributed fused hyperparameters\n",
    "                agent.pxADMM_local_update()                                                                 # local update\n",
    "\n",
    "            # aggregate the objective values, central node\n",
    "            objective_global = 0\n",
    "            for agent in agents:\n",
    "                objective_global += agent.objective_local\n",
    "            objective_log.append(objective_global)\n",
    "\n",
    "            # log the global hyperparameters\n",
    "            gp_hyperparams_log['signal_var'].append(gp_hyperparams_global[0])\n",
    "            gp_hyperparams_log['length_scale'].append(gp_hyperparams_global[1:3])\n",
    "            gp_hyperparams_log['noise_std'].append(gp_hyperparams_global[3])\n",
    "\n",
    "            # check convergence\n",
    "            param_diff = np.linalg.norm(gp_hyperparams_global - old_gp_hyperparams_global)\n",
    "\n",
    "            params_update_log.append(param_diff)\n",
    "        \n",
    "            if param_diff < STOP_TOLERANCE:\n",
    "                is_converged = True\n",
    "            \n",
    "            old_gp_hyperparams_global = copy.deepcopy(gp_hyperparams_global)\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({\"params updated by\": param_diff})\n",
    "\n",
    "    if is_converged:\n",
    "        print('Converged after', itr, 'iterations')\n",
    "    else:\n",
    "        print('Not converged after', MAX_ITR, 'iterations')\n",
    "\n",
    "    print('Final hyperparameters:', param_array_to_dict(gp_hyperparams_global))\n",
    "\n",
    "    return gp_hyperparams_log, objective_log, params_update_log\n",
    "\n",
    "\n",
    "gp_hyperparams_log, objective_log, params_update_log = pxADMM(agent_locations, noisy_samples_reshaped, sample_locations_reshaped, NUM_SAMPLES_PER_AGENT, NUM_AGENTS, MAX_ITR, STOP_TOLERANCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c93df3",
   "metadata": {},
   "source": [
    "### 1.6 Visualize the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f84a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the convergence of the parameters\n",
    "fig, axs = plt.subplots(3, 1, figsize=(8, 6), sharex=True)\n",
    "axs[0].plot(gp_hyperparams_log['signal_var'], label='signal_var')\n",
    "axs[0].axhline(y=gp_hyperparams_true['signal_var'], color='r', linestyle='--', label='true signal_var')\n",
    "axs[0].set_ylabel('signal_var')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot([l[0] for l in gp_hyperparams_log['length_scale']], label='l1')\n",
    "axs[1].plot([l[1] for l in gp_hyperparams_log['length_scale']], label='l2')\n",
    "axs[1].axhline(y=gp_hyperparams_true['length_scale'][0], color='r', linestyle='--', label='true l1')\n",
    "axs[1].axhline(y=gp_hyperparams_true['length_scale'][1], color='g', linestyle='--', label='true l2')\n",
    "axs[1].set_ylabel('length_scale')\n",
    "axs[1].legend()\n",
    "\n",
    "axs[2].plot(gp_hyperparams_log['noise_std'], label='noise_std')\n",
    "axs[2].axhline(y=gp_hyperparams_true['noise_std'], color='r', linestyle='--', label='true noise_std')\n",
    "axs[2].set_ylabel('noise_std')\n",
    "axs[2].legend()\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.show()\n",
    "\n",
    "# visualize the convergence of the objective and the parameters updates\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "axs[0].plot(objective_log)\n",
    "axs[0].set_xlabel('Iteration')\n",
    "axs[0].set_ylabel('Negative Log Likelihood')\n",
    "\n",
    "axs[1].plot(params_update_log)\n",
    "axs[1].set_xlabel('Iteration')\n",
    "axs[1].set_ylabel('Parameters Update')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e1106",
   "metadata": {},
   "source": [
    "You can see that the pxADMM algorithm converges to the true value but at what are the settings and assumptions we used for the algorithm that do not fully represent the  (hint: you can say something about the algorithm initilization)? \n",
    "\n",
    "**Question 6**: Name 2 settings for the above simulation that can be improved.\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b5117",
   "metadata": {},
   "source": [
    "## 3 Algorithm Evaluation (Optional)\n",
    "You've previously used a toy setup to validate the algorithm and now let's give your computer some stress. Recall that one of the motivations for decentralized learning is the complexity reduction. We keep the sample size the same at 5000 but we vary the number of agents (so the size of the local dataset is different) and see the difference in computation time. Do it a few times and see the trend.\n",
    "\n",
    "**Question 6a**: From the printed information, how do the number of iterations change when you vary the number of agents? Reason why this is the case.\n",
    "\n",
    "**Question 6b**: From the printed informtaion, how do the computation time change for each iteration when you vary the number of agents? Reason why this is the case.\n",
    "\n",
    "**Question 6c**: From the plot, what is the trend of the overal computation time when you vary the number of the agents? And explain Why?\n",
    "\n",
    "**Answer**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d818a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "num_agent_list = [20, 50, 100, 150, 200]\n",
    "comutation_time_list = []\n",
    "\n",
    "for NUM_AGENTS in num_agent_list:\n",
    "\n",
    "    NUM_SAMPLES_PER_AGENT = 5000 // NUM_AGENTS\n",
    "\n",
    "    sample_locations_reshaped, sample_locations_list, noisy_samples_reshaped, sample_locations, clean_samples, mesh_x1, mesh_x2 =  generate_data(gp_field, field_range, field_resolution, NUM_AGENTS, NUM_SAMPLES_PER_AGENT, noise_std)\n",
    "\n",
    "    start_time = time.time()\n",
    "    gp_hyperparams_log, objective_log, params_update_log = pxADMM(np.random.rand(GP_INPUT_DIM, NUM_AGENTS), noisy_samples_reshaped, sample_locations_reshaped, NUM_SAMPLES_PER_AGENT, NUM_AGENTS, MAX_ITR, STOP_TOLERANCE)\n",
    "    end_time = time.time()\n",
    "\n",
    "    comutation_time_list.append(end_time - start_time)\n",
    "    print(f\"Time elapsed for NUM_AGENTS={NUM_AGENTS}: {end_time - start_time:.4f} seconds\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(comutation_time_list, 'o-')\n",
    "plt.xlabel('Number of Agents')\n",
    "plt.xticks(range(len(num_agent_list)), num_agent_list)\n",
    "plt.ylabel('Time elapsed (s)')\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195a884",
   "metadata": {},
   "source": [
    "**Feedback**:\n",
    "Your feedback will be invaluable in improving it for the coming years. Please let us know in this markdown any comments, suggestions or errors you have encountered for this assignment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "psf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
